{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing important libraryes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "import nltk\n",
    "#from contractions import contractions_dict\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "from itertools import filterfalse\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"email.csv\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing unwanted labels and remane the lables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.rename(columns={'label_num':'spam'}, inplace=True)\n",
    "data.drop('label',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop('Unnamed: 0',axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unwanted words in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strip_titles(text):\n",
    "    if \"Subject: re :\" in text:\n",
    "        return text[13:]\n",
    "    elif \"Subject: news :\" in text:\n",
    "        return text[15:]\n",
    "    else:\n",
    "        return text[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: strip_titles(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We r going to tolonize all emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalize = Now we have to convert all tokens into lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tokens(list_of_tokens):\n",
    "    return map(lambda x: x.lower(),list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: normalize_tokens(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we r going to remove contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions_dict = {\"'cause\": 'because',\"'tis\": 'it is',\"'twas\": 'it was',\"I'd\": 'I would',\"I'd've\": 'I would have',\"I'll\": 'I will',\"I'll've\": 'I will have',\"I'm\": 'I am',\"I'm'a\": 'I am about to',\"I'm'o\": 'I am going to',\"I've\": 'I have','I’d': 'I would','I’d’ve': 'I would have','I’ll': 'I will','I’ll’ve': 'I will have','I’m': 'I am','I’m’a': 'I am about to','I’m’o': 'I am going to','I’ve': 'I have','Whatcha': 'What are you',\"ain't\": 'are not','ain’t': 'are not',\"amn't\": 'am not','amn’t': 'am not','apr.': 'april',\"aren't\": 'are not','aren’t': 'are not','aug.': 'august',\"can't\": 'cannot',\"can't've\": 'cannot have','can’t': 'cannot','can’t’ve': 'cannot have',\"could've\": 'could have',\"couldn't\": 'could not',\"couldn't've\": 'could not have','couldn’t': 'could not','couldn’t’ve': 'could not have','could’ve': 'could have',\"daren't\": 'dare not','daren’t': 'dare not',\"daresn't\": 'dare not','daresn’t': 'dare not',\"dasn't\": 'dare not','dasn’t': 'dare not','dec.': 'december',\"didn't\": 'did not','didn’t': 'did not',\"doesn't\": 'does not','doesn’t': 'does not',\"don't\": 'do not','don’t': 'do not',\"e'er\": 'ever','em': 'them',\"everyone's\": 'everyone is','everyone’s': 'everyone is','e’er': 'ever','feb.': 'february','finna': 'fixing to','gimme': 'give me',\"gon't\": 'go not','gonna': 'going to','gon’t': 'go not','gotta': 'got to',\"hadn't\": 'had not',\"hadn't've\": 'had not have','hadn’t': 'had not','hadn’t’ve': 'had not have',\"hasn't\": 'has not','hasn’t': 'has not',\"haven't\": 'have not','haven’t': 'have not',\"he'd\": 'he would',\"he'd've\": 'he would have',\"he'll\": 'he will',\"he'll've\": 'he will have',\"he's\": 'he is',\"he've\": 'he have',\"here's\": 'here is','here’s': 'here is','he’d': 'he would','he’d’ve': 'he would have','he’ll': 'he will','he’ll’ve': 'he will have','he’s': 'he is','he’ve': 'he have',\"how'd\": 'how did',\"how'd'y\": 'how do you',\"how'll\": 'how will',\"how're\": 'how are',\"how's\": 'how is','how’d': 'how did','how’d’y': 'how do you','how’ll': 'how will','how’re': 'how are','how’s': 'how is',\"isn't\": 'is not','isn’t': 'is not',\"it'd\": 'it would',\"it'd've\": 'it would have',\"it'll\": 'it will',\"it'll've\": 'it will have',\"it's\": 'it is','it’d': 'it would','it’d’ve': 'it would have','it’ll': 'it will','it’ll’ve': 'it will have','it’s': 'it is','jan.': 'january','jul.': 'july','jun.': 'june','kinda': 'kind of',\"let's\": 'let us','let’s': 'let us','luv': 'love',\"ma'am\": 'madam','mar.': 'march',\"may've\": 'may have',\"mayn't\": 'may not','mayn’t': 'may not','may’ve': 'may have','ma’am': 'madam',\"might've\": 'might have',\"mightn't\": 'might not',\"mightn't've\": 'might not have','mightn’t': 'might not','mightn’t’ve': 'might not have','might’ve': 'might have',\"must've\": 'must have',\"mustn't\": 'must not',\"mustn't've\": 'must not have','mustn’t': 'must not','mustn’t’ve': 'must not have','must’ve': 'must have',\"ne'er\": 'never',\"needn't\": 'need not',\"needn't've\": 'need not have','needn’t': 'need not','needn’t’ve': 'need not have','ne’er': 'never','nov.': 'november',\"o'\": 'of',\"o'clock\": 'of the clock',\"o'er\": 'over','oct.': 'october',\"ol'\": 'old','ol’': 'old',\"oughtn't\": 'ought not',\"oughtn't've\": 'ought not have','oughtn’t': 'ought not','oughtn’t’ve': 'ought not have','o’': 'of','o’clock': 'of the clock','o’er': 'over','sep.': 'september',\"sha'n't\": 'shall not',\"shalln't\": 'shall not','shalln’t': 'shall not',\"shan't\": 'shall not',\"shan't've\": 'shall not have','shan’t': 'shall not','shan’t’ve': 'shall not have','sha’n’t': 'shall not',\"she'd\": 'she would',\"she'd've\": 'she would have',\"she'll\": 'she will',\"she's\": 'she is','she’d': 'she would','she’d’ve': 'she would have','she’ll': 'she will','she’s': 'she is',\"should've\": 'should have',\"shouldn't\": 'should not',\"shouldn't've\": 'should not have','shouldn’t': 'should not','shouldn’t’ve': 'should not have','should’ve': 'should have',\"so's\": 'so is',\"so've\": 'so have',\"somebody's\": 'somebody is','somebody’s': 'somebody is',\"someone's\": 'someone is','someone’s': 'someone is',\"something's\": 'something is','something’s': 'something is','so’s': 'so is','so’ve': 'so have','sux': 'sucks',\"that'd\": 'that would',\"that'd've\": 'that would have',\"that'll\": 'that will',\"that're\": 'that are',\"that's\": 'that is','that’d': 'that would','that’d’ve': 'that would have','that’ll': 'that will','that’re': 'that are','that’s': 'that is',\"there'd\": 'there would',\"there'd've\": 'there would have',\"there'll\": 'there will',\"there're\": 'there are',\"there's\": 'there is','there’d': 'there would','there’d’ve': 'there would have','there’ll': 'there will','there’re': 'there are','there’s': 'there is',\"these're\": 'these are','these’re': 'these are',\"they'd\": 'they would',\"they'd've\": 'they would have',\"they'll\": 'they will',\"they'll've\": 'they will have', \"they're\": 'they are', \"they've\": 'they have', 'they’d': 'they would','they’d’ve': 'they would have', 'they’ll': 'they will', 'they’ll’ve': 'they will have', 'they’re': 'they are','they’ve': 'they have', \"this's\": 'this is', 'this’s': 'this is', \"those're\": 'those are','those’re': 'those are', \"to've\": 'to have', 'to’ve': 'to have', 'wanna': 'want to',\"wasn't\": 'was not', 'wasn’t': 'was not', \"we'd\": 'we would', \"we'd've\": 'we would have',\"we'll\": 'we will', \"we'll've\": 'we will have', \"we're\": 'we are', \"we've\": 'we have',\"weren't\": 'were not', 'weren’t': 'were not', 'we’d': 'we would', 'we’d’ve': 'we would have','we’ll': 'we will', 'we’ll’ve': 'we will have', 'we’re': 'we are', 'we’ve': 'we have',\"what'd\": 'what did', \"what'll\": 'what will', \"what'll've\": 'what will have',\"what're\": 'what are', \"what's\": 'what is', \"what've\": 'what have', 'what’d': 'what did','what’ll': 'what will', 'what’ll’ve': 'what will have', 'what’re': 'what are', 'what’s': 'what is','what’ve': 'what have', \"when's\": 'when is', \"when've\": 'when have', 'when’s': 'when is', 'when’ve': 'when have', \"where'd\": 'where did',\"where're\": 'where are', \"where's\": 'where is', \"where've\": 'where have', 'where’d': 'where did','where’re': 'where are', 'where’s': 'where is', 'where’ve': 'where have', \"which's\": 'which is','which’s': 'which is', \"who'd\": 'who would', \"who'd've\": 'who would have', \"who'll\": 'who will',\"who'll've\": 'who will have', \"who're\": 'who are', \"who's\": 'who is', \"who've\": 'who have','who’d': 'who would', 'who’d’ve': 'who would have', 'who’ll': 'who will', 'who’ll’ve': 'who will have','who’re': 'who are', 'who’s': 'who is', 'who’ve': 'who have', \"why'd\": 'why did',\"why're\": 'why are', \"why's\": 'why is', \"why've\": 'why have', 'why’d': 'why did','why’re': 'why are', 'why’s': 'why is', 'why’ve': 'why have', \"will've\": 'will have','will’ve': 'will have', \"won't\": 'will not', \"won't've\": 'will not have', 'won’t': 'will not','won’t’ve': 'will not have', \"would've\": 'would have', \"wouldn't\": 'would not', \"wouldn't've\": 'would not have','wouldn’t': 'would not', 'wouldn’t’ve': 'would not have', 'would’ve': 'would have', \"y'all\": 'you all', \"y'all'd\": 'you all would', \"y'all'd've\": 'you all would have',\"y'all're\": 'you all are', \"y'all've\": 'you all have', \"you'd\": 'you would', \"you'd've\": 'you would have', \"you'll\": 'you will', \"you'll've\": 'you shall have', \"you're\": 'you are', \"you've\": 'you have','you’d': 'you would', 'you’d’ve': 'you would have', 'you’ll': 'you will', 'you’ll’ve': 'you shall have', 'you’re': 'you are', 'you’ve': 'you have', 'y’all': 'you all', 'y’all’d': 'you all would', 'y’all’d’ve': 'you all would have','y’all’re': 'you all are', 'y’all’ve': 'you all have', '’cause': 'because', '’tis': 'it is', '’twas': 'it was'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contracted_word_expansion(token):\n",
    "    if token in contractions_dict.keys():\n",
    "        return contractions_dict[token]\n",
    "    else:\n",
    "        return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contracted_word_expansion('You don\\'t need a library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contractions_expansion(list_of_tokens):\n",
    "    return map(contracted_word_expansion,list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: contractions_expansion(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now data came is a non contracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we remove unnecessary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex = r'^@[a-zA-z0-9]|^#[a-zA-Z0-9]|\\w+:\\/{2}[\\d\\w-]+(\\.[\\d\\w-]+)*(?:(?:\\/[^\\s/]*))*|\\W+|\\d+|<(\"[^\"]*\"|\\'[^\\']*\\'|[^\\'\">])*>|_+|[^\\u0000-\\u007f]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def waste_word_or_not(token):\n",
    "    return re.search(regex,token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_waste_words(list_of_tokens):\n",
    "    return filterfalse(waste_word_or_not,list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: filter_waste_words(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(list_of_tokens):\n",
    "    return map(lambda x: re.split(regex,x)[0],list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: split(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We r going to remove stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_stop_words = list(set(stopwords.words('english')).union(set(STOP_WORDS)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(en_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_stopword(token):\n",
    "    return not(token in en_stop_words or re.search(r'\\b\\w\\b|[^\\u0000-\\u007f]+|_+|\\W+',token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stopwords_removals(list_of_tokens):\n",
    "    return filter(is_stopword,list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: stopwords_removals(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we r going to perform limitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wnet_pos_tag(treebank_tag):\n",
    "    if treebank_tag[1].startswith('J'):\n",
    "        return (treebank_tag[0],wordnet.ADJ)\n",
    "    elif treebank_tag[1].startswith('V'):\n",
    "        return (treebank_tag[0],wordnet.VERB)\n",
    "    elif treebank_tag[1].startswith('N'):\n",
    "        return (treebank_tag[0],wordnet.NOUN)\n",
    "    elif treebank_tag[1].startswith('R'):\n",
    "        return (treebank_tag[0],wordnet.ADV)\n",
    "    else:\n",
    "        (treebank_tag[0],wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pos_tag(list_of_tokens):\n",
    "    return map(get_wnet_pos_tag,pos_tag(list_of_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: get_pos_tag(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we r going to apply Limitization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_lemmatization(token):\n",
    "    if token == None:\n",
    "        return \"\"\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word=token[0],pos=token[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(list_of_tokens):\n",
    "    if len(list_of_tokens) > 0:\n",
    "        return map(lambda x: token_lemmatization(x),list_of_tokens)\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: lemmatization(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: list(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()\n",
    "for list_of_tokens in data['text']:\n",
    "    vocab = vocab.union(set(list_of_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = list(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict = dict(zip(vocab,list(range(0,len(vocab)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we r joinng tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_tokens(list_of_tokens):\n",
    "    return \" \".join(list_of_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['text'] = data['text'].apply(lambda x: join_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list()\n",
    "for email_text in data['text']:\n",
    "    corpus.append(email_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(vocabulary=vocab_dict)\n",
    "tf_idf_matrix = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix = tf_idf_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam'] = data['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix_reduced = pca.fit_transform(tf_idf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=tf_idf_matrix_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['spam'] = data['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.iloc[:,0:5000]\n",
    "y_train = df['spam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X=X_train,y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_categories = gnb.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_true=y_train,y_pred=predicted_categories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
